{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29eb7f8-1593-48db-aaba-80fefe541053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import tweepy\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast, GPT2Tokenizer, DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca1d368-2550-4aab-9b0d-8e9a3313dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Parameters\n",
    "n = 799000  # Number of rows to remove from the start and end\n",
    "csv_path = 'encoded-training.1600000.processed.noemoticon.csv'  # Path to your CSV file\n",
    "output_path = 'reduced-encoded-training.1600000.processed.noemoticon.csv'  # Path for the modified CSV file\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Step 2: Check the number of rows\n",
    "if len(df) <= 2 * n:\n",
    "    raise ValueError(\"The DataFrame is too small to remove that many rows.\")\n",
    "\n",
    "# Step 3: Drop the first n and last n rows\n",
    "df_modified = df.iloc[n:-n]\n",
    "\n",
    "df_modified = df_modified.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Save the modified DataFrame\n",
    "df_modified.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6628d9-51e7-4990-9a02-6a365353224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment          id                          date     query  \\\n",
      "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1         0  2329204651  Thu Jun 25 10:28:26 PDT 2009  NO_QUERY   \n",
      "2         0  2329027428  Thu Jun 25 10:15:53 PDT 2009  NO_QUERY   \n",
      "3         4  1467862699  Mon Apr 06 22:33:21 PDT 2009  NO_QUERY   \n",
      "4         0  2329144318  Thu Jun 25 10:24:09 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    Badmantalking  and I think it has a mind of its own,like it a...  \n",
      "2          Stalkum  2 hours to rest in my house and of return to t...  \n",
      "3           mlanet  just got home from Neil's. watched Nick &amp; ...  \n",
      "4    BrookeLogan09  @wwjpat am sure he'll get &quot;there&quot; in...  \n"
     ]
    }
   ],
   "source": [
    "columns = ['sentiment','id','date','query','user','text']\n",
    "dataset_path = 'reduced-encoded-training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(dataset_path, header = None, names = columns, encoding = 'utf-8', dtype ={0:str}, low_memory=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18074a6-5b70-4783-89aa-a0a6498b66ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sentiment          id                          date     query  \\\n",
      "0         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1         0  2329204651  Thu Jun 25 10:28:26 PDT 2009  NO_QUERY   \n",
      "2         0  2329027428  Thu Jun 25 10:15:53 PDT 2009  NO_QUERY   \n",
      "3         4  1467862699  Mon Apr 06 22:33:21 PDT 2009  NO_QUERY   \n",
      "4         0  2329144318  Thu Jun 25 10:24:09 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  switchfoot  - Awww, that's a bummer.  You shou...  \n",
      "1    Badmantalking  and I think it has a mind of its own,like it a...  \n",
      "2          Stalkum  2 hours to rest in my house and of return to t...  \n",
      "3           mlanet  just got home from Neil's. watched Nick &amp; ...  \n",
      "4    BrookeLogan09  wwjpat am sure he'll get &quot;there&quot; in ...  \n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweets(text):\n",
    "    text  = re.sub(r\"http\\S+|www.\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@|\\#','', text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_tweets)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622da83e-764c-4250-8d95-9bd5e387a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Tokenizing the tweets\n",
    "#df['text'] = df['text'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11302f42-2880-46a2-919c-8ec4d99b38b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    500\n",
      "1    500\n",
      "Name: sentiment, dtype: int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# Convert sentiment labels, assuming 0 is negative and 4 is positive\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "df['sentiment'] = df['sentiment'].replace(4,1)\n",
    "df['sentiment'] = df['sentiment'].replace(0,0)\n",
    "\n",
    "# Example conversion, adjust based on your actual labels\n",
    "#df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "print(df['sentiment'].value_counts())\n",
    "print(df['sentiment'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d193807-16d4-4839-8597-4a52a6d9264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training and test sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb358a29-b312-4440-b0ce-417f905d2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_twitter_dataset import SentimentDataset\n",
    "      \n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "MAX_LEN = 100\n",
    "\n",
    "#dataset = TwitterDataset(df['text'].to_numpy(), df['sentiment'].to_numpy(), tokenizer, MAX_LEN)\n",
    "        \n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69853aa8-c4f5-496c-81d0-40a30a387841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_data_loader(df, max_len, batch_size):\n",
    "  ds = SentimentDataset(\n",
    "    texts=df['text'].to_numpy(),\n",
    "    labels=df['sentiment'].to_numpy(),\n",
    "    #tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=8\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 900\n",
    "\n",
    "#data_loader = create_data_loader(df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "train_data_loader = create_data_loader(train_df, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4447f63d-a509-40a1-a20d-224a9cf9dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BertForSentimentAnalysis(nn.Module):\n",
    "    def __init__(self, freeze_bert=True):\n",
    "        super(BertForSentimentAnalysis, self).__init__()\n",
    "        # Instantiating BERT model object \n",
    "        self.bert_layer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        # Freeze BERT layers to prevent training (optional)\n",
    "        if freeze_bert:\n",
    "            for param in self.bert_layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        print(\"hii\")\n",
    "        # Classification layer\n",
    "        #self.cls_layer = nn.Linear(768, 1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.bert_layer.config.hidden_size, 8),\n",
    "            nn.BatchNorm1d(8),  # Batch Normalization\n",
    "            nn.LeakyReLU(0.5),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(8, 1)\n",
    ")\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass inputs through BERT\n",
    "        outputs = self.bert_layer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Apply classification layers\n",
    "        logits = self.classifier(last_hidden_state[:, 0, :])\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8d6fece-f903-4b70-bb64-686f926315f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForSentimentAnalysis()\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# Tokenize your data here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c352e4a-9be5-4204-b645-e2501aa45ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 4  # Example: Accumulate gradients over 4 forward passes\n",
    "def train(model, data_loader, criterion, optimizer, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "\n",
    "    steps_accumulated = 0\n",
    "    \n",
    "    with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "\n",
    "        for batch in data_loader:\n",
    "            print(\"hello\")\n",
    "            # Move batch data to the device (CPU/GPU)\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels'].float()   \n",
    "\n",
    "            # Reset gradients\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass, get predictions\n",
    "            outputs = model(input_ids, attention_mask).squeeze(1)\n",
    "            # Calculate loss and scale it\n",
    "            loss = criterion(outputs.squeeze(-1), labels) / accumulation_steps\n",
    "            total_loss += loss.item() * accumulation_steps  # Scale back up\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            steps_accumulated += 1\n",
    "\n",
    "            if steps_accumulated == accumulation_steps:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                steps_accumulated = 0\n",
    "                \n",
    "            acc = binary_accuracy(outputs, labels) \n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    \n",
    "    return total_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    epoch_acc = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Move batch data to the device (CPU/GPU)\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels'].float()\n",
    "            \n",
    "            \n",
    "            # Forward pass, get predictions\n",
    "            outputs = model(input_ids, attention_mask).squeeze(1)\n",
    "\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs.squeeze(-1), labels)\n",
    "            acc = binary_accuracy(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    #epoch_acc = total_correct / total_samples      \n",
    "        \n",
    "    return total_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b6c9d1-b715-4768-9e7f-7029cf2e24cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e4a40-28b2-4492-9920-aa70de8b5012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2  # Number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    train_loss, train_acc = train(model, train_data_loader, criterion, optimizer, accumulation_steps )\n",
    "    val_loss, val_acc = evaluate(model, val_data_loader, criterion )\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} - Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}, Val Loss: {val_loss:.3f}|  Val. Acc: {val_acc*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7af3e-f549-40ec-ae13-36bf926e8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_data_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720bfe7c-dce4-4783-9b08-8aacec7579f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5140c46-e246-4e27-9e08-b2f3c8993914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbad161-9639-4c7d-bd17-d473933e0277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
