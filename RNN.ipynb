{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a317edc-7ae8-4b67-ac33-eb437c04fdbc",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6209c1-d41b-4bed-88c1-45aa91bc8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import tweepy\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertTokenizerFast, GPT2Tokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75544c04-db14-4022-8474-981978d95dae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa96a6-14a6-4c10-a0f4-986864e6c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentiment','id','date','query','user','text']\n",
    "dataset_path = 'encoded-training.1600000.processed.noemoticon.csv'\n",
    "df = pd.read_csv(dataset_path, header = None, names = columns, encoding = 'utf-8', dtype ={0:str}, low_memory=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cc15bf-74f4-4089-ae97-eba01ad19254",
   "metadata": {},
   "source": [
    "# Pre Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c2efb-1fe9-45ee-9109-2b026d0c1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweets(text):\n",
    "    text  = re.sub(r\"http\\S+|www.\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@|\\#','', text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136cea8-0e68-4ae5-a4e3-29732481ca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727d65c-3f76-4e5f-b5ce-0c1cb8bf02f3",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cadab71-74d1-4638-acaa-c103bba50332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Tokenizing the tweets\n",
    "#df['text'] = df['text'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f19547-6c94-489c-9e96-22cc651a517d",
   "metadata": {},
   "source": [
    "# Converting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be7890-fb65-494c-9c72-08477cb84e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment labels, assuming 0 is negative and 4 is positive\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "df['sentiment'] = df['sentiment'].replace(4,1)\n",
    "df['sentiment'] = df['sentiment'].replace(0,0)\n",
    "\n",
    "# Example conversion, adjust based on your actual labels\n",
    "#df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "print(df['sentiment'].value_counts())\n",
    "print(df['sentiment'].dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff207e-06bc-4fc6-8bc3-8be56efa9132",
   "metadata": {},
   "source": [
    "# Splitting Data into Train, Test and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa21970c-3711-428f-9400-7632820bf487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset into training and test sets (80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training set into training and validation sets (80% train, 20% validation)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e913d50-93c3-4ccf-b1f8-b7014a1c61c9",
   "metadata": {},
   "source": [
    "# Implementing Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24e0805-a1b6-47e5-bef6-3ddf9d8e4666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn_twitter_dataset import TwitterDataset\n",
    "      \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_LEN = 250\n",
    "                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbab43-5b5d-4b93-8aac-d0deac3f9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = TwitterDataset(\n",
    "    tweets=df['text'].to_numpy(),\n",
    "    labels=df['sentiment'].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=8\n",
    "  )\n",
    "\n",
    "BATCH_SIZE = 16000\n",
    "\n",
    "#data_loader = create_data_loader(df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "train_data_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ff2d8-e51e-443b-a740-d3d7ebdef2d4",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb7517-574c-43cb-b744-699819528578",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text,lengths, attention_mask):\n",
    "         \n",
    "        lengths, sorted_idx = lengths.sort(0, descending=True)\n",
    "        text = text[sorted_idx]\n",
    "        attention_mask = attention_mask[sorted_idx]\n",
    "        \n",
    "        # Pack sequence\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True)\n",
    "        packed_output, (hidden, _) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "# Model hyperparameters (you should tune these)\n",
    "VOCAB_SIZE = len(tokenizer)  # Adjust based on your tokenizer\n",
    "EMBEDDING_DIM = 12\n",
    "HIDDEN_DIM = 8\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = False\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = SentimentRNN(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c76876-1824-4618-98fa-ff287b1f3de0",
   "metadata": {},
   "source": [
    "# Creating Training and Evaluating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d04d2-fbcc-45f6-ae04-a4fee40bfa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        #print('hello')\n",
    "        optimizer.zero_grad()\n",
    "        #text, text_lengths = batch.text\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        lengths = batch['length'] \n",
    "        labels = batch['labels']\n",
    "        predictions = model(input_ids, lengths,attention_mask).squeeze(1)\n",
    "        labels = labels.float()\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            lengths = batch['length'] \n",
    "            labels = batch['labels']\n",
    "            predictions = model(input_ids, lengths,attention_mask).squeeze(1)\n",
    "            labels = labels.float()\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5968d7-5cfc-4cbc-84c6-21c25f20b568",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfbd93-1a6b-4d11-8c09-dd5760eac372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5  # Number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(model, train_data_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_data_loader, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a7f50-50b2-40a2-9a89-7c8f341074ba",
   "metadata": {},
   "source": [
    "# Final Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf3d07-8042-4527-b05b-753ae7b2129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_data_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d290b66-7dcd-4a45-a328-7a1266239085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca9aa5-111c-45ec-8e5e-a7149392d166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
